AI Agent RAG System - Technical Overview

This document describes the RAG (Retrieval-Augmented Generation) system integrated into the AI Agent application.

## Architecture

The RAG system follows a retrieval-before-tools principle, meaning document knowledge is always consulted before any external tool invocations. This is implemented as a LangGraph subgraph that serves as the entry point to the main agent graph.

## Key Components

### 1. Vector Database
We use ChromaDB as our vector database with a single collection architecture. Documents are isolated per user using metadata filtering with user_id fields. This provides efficient multi-tenancy without the overhead of managing separate collections.

### 2. Embedding Model
The system uses OpenAI's text-embedding-3-small model, which provides 1536-dimensional embeddings. This model offers an excellent balance between cost, speed, and quality for most use cases.

### 3. Document Chunking
Text documents are split into chunks of 500-700 tokens with a 10-15% overlap. The chunking is paragraph-aware, respecting natural document boundaries like double newlines and Markdown headings. This preserves semantic coherence in retrieved passages.

### 4. RAG Pipeline (5 Nodes)

The RAG pipeline consists of five sequential LangGraph nodes:

**QueryRewriteNode**: Optimizes the user's query for better retrieval by expanding abbreviations, normalizing language, and considering user preferences like language and default city.

**RetrieveNode**: Performs vector similarity search in ChromaDB, filtering results by user_id and returning the top-k most relevant chunks with similarity scores.

**ContextBuilderNode**: Converts retrieved chunks into prompt-ready context, enforcing a token budget of 2500 tokens and generating citation labels like [RAG-1], [RAG-2] for each chunk.

**GuardrailNode**: Validates the retrieval results, checks citation formatting, and sets flags to indicate whether knowledge is available for the agent.

**FeedbackNode**: Collects and aggregates performance metrics including query rewrite latency, retrieval latency, and similarity scores.

### 5. Citation Enforcement

The agent is prompted to ALWAYS cite sources when using retrieved knowledge. Citations appear as [RAG-1], [RAG-2] in responses. This ensures transparency and allows users to trace information back to source documents.

## API Endpoints

Four RESTful endpoints support RAG operations:
- POST /api/rag/upload: Upload .txt or .md files
- GET /api/rag/stats/{user_id}: Get document and chunk counts
- GET /api/rag/documents/{user_id}: List all documents
- DELETE /api/rag/documents/{doc_id}: Delete a document and its chunks

## Frontend Integration

The frontend includes a DocumentUpload component in the sidebar for file management, displays RAG statistics in the header (document count, chunk count), and shows detailed RAG context and metrics in the Debug Panel including retrieved chunk previews and performance measurements.

## Performance Considerations

The system tracks latency at each pipeline stage. Query rewriting typically takes 100-300ms, while retrieval from ChromaDB takes 50-150ms depending on collection size. The entire pipeline usually completes in under 500ms, ensuring responsive user experience.

## Future Enhancements

Potential improvements include adding a reranking stage with cross-encoder models for better precision, implementing hybrid search combining BM25 and vector search, supporting additional document formats like PDF and DOCX, and adding query expansion to generate multiple query variations for improved recall.
