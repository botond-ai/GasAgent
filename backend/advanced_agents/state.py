"""
Advanced Agent State Models with Reducers.

This module defines TypedDict state models for complex agent workflows:
- Plan-and-Execute state tracking
- Parallel execution coordination
- Dynamic routing decisions
- Result aggregation patterns

WHY TypedDict over Pydantic here?
- LangGraph uses TypedDict for state by default
- Better integration with LangGraph's built-in state management
- More explicit about what fields are mutable during graph execution
- Reducers work seamlessly with TypedDict

WHY Reducers?
- Safe merging of parallel execution results
- Deterministic state updates from multiple sources
- Avoid race conditions and data loss
- Enable true parallel node execution in LangGraph

Following SOLID: Single Responsibility - each model represents one concept.
"""

from typing import List, Dict, Any, Optional, Sequence, Annotated
from typing_extensions import TypedDict
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_core.messages import BaseMessage


# ============================================================================
# REDUCER FUNCTIONS - Used by LangGraph to merge state from parallel nodes
# ============================================================================

def list_reducer(existing: List[Any], new: List[Any]) -> List[Any]:
    """
    Merge two lists by appending new items to existing.
    
    WHY: When multiple parallel nodes emit results, we want to collect ALL of them.
    Example: 3 parallel API calls each return results → we want all 3 in final state.
    
    Args:
        existing: Current list in state
        new: New list from a node
        
    Returns:
        Combined list with all items
    """
    return existing + new


def dict_merge_reducer(existing: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:
    """
    Merge two dictionaries, with new values overwriting existing for same keys.
    
    WHY: Parallel nodes may produce different keys (weather → temp, news → headline).
    We want to combine all keys without losing data.
    
    Args:
        existing: Current dictionary in state
        new: New dictionary from a node
        
    Returns:
        Merged dictionary with all keys
    """
    merged = existing.copy()
    merged.update(new)
    return merged


def parallel_results_reducer(
    existing: List[Dict[str, Any]], 
    new: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Merge results from parallel task executions, preserving task identity.
    
    WHY: When tasks run in parallel, we need to:
    1. Collect all results
    2. Preserve which result came from which task
    3. Handle failures gracefully
    
    Args:
        existing: Current list of task results
        new: New task results from parallel execution
        
    Returns:
        Combined list of all task results
    """
    # Use task_id as unique identifier to avoid duplicates
    existing_ids = {r.get("task_id") for r in existing if "task_id" in r}
    
    # Add only new results not already in existing
    for result in new:
        if result.get("task_id") not in existing_ids:
            existing.append(result)
    
    return existing


# ============================================================================
# PYDANTIC MODELS - For structured LLM outputs and validation
# ============================================================================

class PlanStep(BaseModel):
    """
    A single step in an execution plan.
    
    WHY separate from state?
    - LLM outputs this as JSON (needs Pydantic validation)
    - Clear schema for LLM to follow
    - Easy to serialize/deserialize
    """
    step_id: str = Field(description="Unique identifier for this step (e.g., 'step_1')")
    description: str = Field(description="Human-readable description of what this step does")
    tool_name: str = Field(description="Name of the tool to execute (e.g., 'weather', 'fx_rates')")
    arguments: Dict[str, Any] = Field(description="Arguments to pass to the tool")
    depends_on: List[str] = Field(
        default_factory=list,
        description="List of step_ids that must complete before this step can run"
    )
    can_run_parallel: bool = Field(
        default=False,
        description="Whether this step can run in parallel with others"
    )


class ExecutionPlan(BaseModel):
    """
    Complete execution plan generated by the planner.
    
    WHY a plan object?
    - Makes the agent's thinking explicit and inspectable
    - Enables plan modification before execution
    - Better debugging and observability
    - Users can see what the agent intends to do
    """
    plan_id: str = Field(description="Unique identifier for this plan")
    goal: str = Field(description="High-level goal this plan achieves")
    steps: List[PlanStep] = Field(description="Ordered list of steps to execute")
    estimated_duration_seconds: Optional[float] = Field(
        default=None,
        description="Estimated time to complete all steps"
    )
    created_at: datetime = Field(default_factory=datetime.now)


class ParallelTask(BaseModel):
    """
    A task to be executed in parallel with others.
    
    WHY separate task model?
    - Clear boundary between sequential and parallel execution
    - Tracks task identity for result matching
    - Enables independent error handling per task
    """
    task_id: str = Field(description="Unique identifier for this parallel task")
    task_type: str = Field(description="Type of task (e.g., 'api_call', 'computation')")
    tool_name: str = Field(description="Tool to execute")
    arguments: Dict[str, Any] = Field(description="Tool arguments")
    timeout_seconds: float = Field(default=30.0, description="Max execution time")


class AggregationResult(BaseModel):
    """
    Result of aggregating multiple parallel task results.
    
    WHY aggregate?
    - Multiple parallel results need to be combined into one answer
    - Need to handle partial failures (2 of 3 tasks succeeded)
    - Provide summary statistics for debugging
    """
    total_tasks: int = Field(description="Total number of parallel tasks")
    successful_tasks: int = Field(description="Number of tasks that succeeded")
    failed_tasks: int = Field(description="Number of tasks that failed")
    aggregated_data: Dict[str, Any] = Field(
        description="Combined data from all successful tasks"
    )
    errors: List[Dict[str, str]] = Field(
        default_factory=list,
        description="Errors from failed tasks"
    )
    execution_time_ms: float = Field(description="Total time for parallel execution")


# ============================================================================
# LANGGRAPH STATE - TypedDict with Annotated reducers
# ============================================================================

class AdvancedAgentState(TypedDict, total=False):
    """
    LangGraph state for advanced agent workflows.
    
    WHY so many fields?
    - Planning requires tracking plan + current step
    - Parallel execution requires task queue + results
    - Routing requires decision state
    - Aggregation requires collected results
    
    WHY Annotated with reducers?
    - LangGraph uses reducers to merge state from parallel nodes
    - Without reducers, parallel results would overwrite each other
    - Annotated[] tells LangGraph HOW to merge specific fields
    
    Example: Two parallel nodes both emit parallel_results:
        Node A: parallel_results = [{"task_id": "1", "data": "A"}]
        Node B: parallel_results = [{"task_id": "2", "data": "B"}]
        With reducer: parallel_results = [{"task_id": "1", ...}, {"task_id": "2", ...}]
        Without reducer: parallel_results = [{"task_id": "2", "data": "B"}] (lost A!)
    """
    
    # Core message flow
    messages: Annotated[Sequence[BaseMessage], list_reducer]
    
    # User context
    current_user_id: str
    session_id: Optional[str]
    
    # Plan-and-Execute fields
    execution_plan: Optional[ExecutionPlan]  # The plan generated by planner
    current_step_index: int  # Which step we're currently executing
    plan_completed: bool  # Whether all steps are done
    plan_results: Annotated[List[Dict[str, Any]], list_reducer]  # Results from each step
    
    # Parallel execution fields
    parallel_tasks: List[ParallelTask]  # Tasks to run in parallel
    parallel_results: Annotated[List[Dict[str, Any]], parallel_results_reducer]  # Results
    parallel_execution_active: bool  # Whether parallel execution is ongoing
    
    # Routing fields
    routing_decision: Optional[Dict[str, Any]]  # Which nodes to execute next
    next_nodes: List[str]  # Node names to route to (can be multiple for parallel)
    
    # Aggregation fields
    aggregation_result: Optional[AggregationResult]  # Final aggregated result
    aggregated_data: Annotated[Dict[str, Any], dict_merge_reducer]  # Merged data
    
    # Iteration control
    iteration_count: int  # Prevent infinite loops
    max_iterations: int  # Safety limit
    
    # Debug and observability
    debug_logs: Annotated[List[str], list_reducer]  # Educational debug output
    tools_called: Annotated[List[Dict[str, Any]], list_reducer]  # Tool call history
    
    # Final output
    final_answer: Optional[str]  # The final response to the user


# ============================================================================
# HELPER FUNCTIONS - State initialization and validation
# ============================================================================

def create_initial_state(user_id: str, message: str, session_id: Optional[str] = None) -> AdvancedAgentState:
    """
    Create a properly initialized AdvancedAgentState.
    
    WHY a factory function?
    - Ensures all required fields are set
    - Provides sensible defaults
    - Reduces boilerplate in calling code
    
    Args:
        user_id: User identifier
        message: User's input message
        session_id: Optional session identifier
        
    Returns:
        Fully initialized state ready for graph execution
    """
    from langchain_core.messages import HumanMessage
    
    return AdvancedAgentState(
        messages=[HumanMessage(content=message)],
        current_user_id=user_id,
        session_id=session_id,
        execution_plan=None,
        current_step_index=0,
        plan_completed=False,
        plan_results=[],
        parallel_tasks=[],
        parallel_results=[],
        parallel_execution_active=False,
        routing_decision=None,
        next_nodes=[],
        aggregation_result=None,
        aggregated_data={},
        iteration_count=0,
        max_iterations=20,
        debug_logs=[],
        tools_called=[],
        final_answer=None
    )
