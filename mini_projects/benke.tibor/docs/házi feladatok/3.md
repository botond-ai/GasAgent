Az API h√≠v√°sok m√°r teljes m√©rt√©kben LangGraph alap√∫ak.

## LangGraph alap√∫ m≈±k√∂d√©s:
üîÑ Mi t√∂rt√©nik amikor IT domain query √©rkezik:
POST /api/query/ ‚Üí LangGraph StateGraph workflow ind√≠t√°sa

7 node v√©grehajt√≥dik:

intent_detection ‚Üí domain = "it"
retrieval ‚Üí Qdrant RAG (IT-KB dokumentumok)
generation ‚Üí GPT-4o-mini v√°lasz cit√°ci√≥kkal
guardrail ‚Üí Valid√°l√°s (IT-KB-XXX form√°tum ellen≈ërz√©s)
collect_metrics ‚Üí Telemetria
execute_workflow üëà Itt k√©sz√ºl a Jira ticket draft
memory_update ‚Üí Session ment√©s
Workflow node (6. node) el≈ëk√©sz√≠ti a Jira ticket adatokat:

Frontend megkapja a workflow adatot ‚Üí a felhaszn√°l√≥ igen-t v√°lasztja ‚Üí POST /api/jira/ticket/ ‚Üí T√©nyleges Jira API h√≠v√°s

‚úÖ Mi√©rt k√©t l√©p√©s (draft + creation)?
Design d√∂nt√©s:

üéØ User control: Megn√©zheti a draft-ot l√©trehoz√°s el≈ëtt
üîí No side-effects: LangGraph workflow idempotens (√∫jra futtathat√≥)
üõ°Ô∏è Error handling: Jira API failure nem befoly√°solja a query v√°laszt
üìù Audit: K√ºl√∂n log entry a ticket creation-r≈ël
# R√©szletes vizu√°lis flow diagram √©s magyar√°zat: LANGGRAPH_API_FLOW.md 

## Pydentic valid√°ci√≥
1. IntentOutput integr√°ci√≥ (agent.py)
‚ùå El≈ëtte: response.content.strip().lower() manual parsing
‚úÖ Ut√°na: llm.with_structured_output(IntentOutput) - automatikus valid√°ci√≥
El≈ëny√∂k:
Confidence √©rt√©ket kerek√≠ti 3 tizedesre
Figyelmeztet ha confidence < 0.5
Reasoning mez≈ë 10-500 karakter hossz√∫s√°g valid√°ci√≥
2. RAGGenerationOutput integr√°ci√≥ (agent.py)
‚ùå El≈ëtte: response.content egyszer≈± string
‚úÖ Ut√°na: llm.with_structured_output(RAGGenerationOutput) - struktur√°lt kimenet
El≈ëny√∂k:
Answer minimum 10 karakter
Language default "hu" ha nem adott
Section_ids valid√°ci√≥ regex pattern ^[A-Z]+-KB-\d+$
Automatikus √∂sszekapcsol√°s az extrah√°lt section_ids-ekkel
3. MemoryUpdate integr√°ci√≥ (agent.py)
‚ùå El≈ëtte: K√©t k√ºl√∂n LLM h√≠v√°s (summary + facts), manual parsing, duplik√°ci√≥ check
‚úÖ Ut√°na: Egy llm.with_structured_output(MemoryUpdate) h√≠v√°s
El≈ëny√∂k:
model_validator biztos√≠tja, hogy legal√°bb 1 tartalomt√≠pus (summary/facts/decisions) l√©tezik
Automatikus facts deduplik√°ci√≥ (egyedi list√°k)
Egyszer≈±bb k√≥d, kevesebb hibalehet≈ës√©g
4. TurnMetrics integr√°ci√≥ (agent.py)
‚ùå El≈ëtte: Manual dict construction
‚úÖ Ut√°na: TurnMetrics(...) Pydantic modell
El≈ëny√∂k:
llm_latency_ms > 30000 eset√©n warning log
Datetime automatikus JSON encoding (ISO 8601 format)
Type safety minden metric mez≈ëre
Valid√°lt output a JSONL logging sz√°m√°ra

## Hibakezel√©sek
# HTTP status codes LangGraph √°llapot alapj√°n 
 HTTP Status Code p√©ld√°k:
Scenario	HTTP Code	Processing Status	Success Flag
‚úÖ Clean pipeline	200 OK	SUCCESS	true
‚ö†Ô∏è Guardrail retry ‚Üí success	206 Partial Content	PARTIAL_SUCCESS	true
‚ùå RAG timeout/failure	503 Service Unavailable	RAG_UNAVAILABLE	false
‚ùå LLM generation error	500 Internal Server Error	GENERATION_FAILED	false
‚ùå Max validation retries	422 Unprocessable Entity	VALIDATION_FAILED	false

# timeout kezel√©s + fallback + retry + state tracking
OPENAI_TIMEOUT = int(os.getenv('OPENAI_TIMEOUT', 30))  # seconds
OPENAI_MAX_RETRIES = int(os.getenv('OPENAI_MAX_RETRIES', 3))
RAG_TIMEOUT = int(os.getenv('RAG_TIMEOUT', 10))  # seconds

Async Timeout Wrapper (error_handling.py)
Kezel√©si logika:
‚úÖ asyncio.TimeoutError ‚Üí Exponential backoff retry (1s, 2s, 4s...)
‚úÖ RateLimitError, APIConnectionError, APIError ‚Üí Retry with backoff
‚úÖ Max retries el√©r√©se ‚Üí TimeoutError vagy APICallError exception
üìä Logging minden attempt-n√©l

RAG Timeout Integration (agent.py)
Timeout strat√©gia:
Komponens	Timeout	Max Retries	Exponential Backoff	Fallback
RAG (Qdrant)	10s	3	1s ‚Üí 2s ‚Üí 4s	Empty citations + rag_unavailable flag
LLM (OpenAI)	30s	3	1s ‚Üí 2s ‚Üí 4s	(k√∂vetkez≈ë feladat)
Memory Update	30s	3	1s ‚Üí 2s ‚Üí 4s	Non-blocking error

üîÑ Retry logika:
Attempt 1 (timeout: 10s)
   ‚Üì TIMEOUT
Wait 1.0s
   ‚Üì
Attempt 2 (timeout: 10s)
   ‚Üì TIMEOUT
Wait 2.0s
   ‚Üì
Attempt 3 (timeout: 10s)
   ‚Üì TIMEOUT
Raise TimeoutError ‚Üí RAG unavailable ‚Üí Summary-only mode

State tracking:
RAG unavailable eset√©n:
state["rag_unavailable"] = True  # Flag be√°ll√≠t√°s
processing_status = ProcessingStatus.RAG_UNAVAILABLE  # HTTP 503

# Degrad√°ci√≥: RAG unavailable ‚Üí summary-only mode
Graceful Degradation implement√°lva:
1. RAG unavailable detection (agent.py)
2. Generation node fallback check (agent.py)
Summary-only response generator (agent.py)
√öj met√≥dus: _generate_summary_only_response()

Adatforr√°sok:
‚úÖ memory_summary - Kor√°bbi besz√©lget√©s √∂sszefoglal√°sa
‚úÖ memory_facts - Extrah√°lt t√©nyek (max 5)
‚úÖ messages[-5:] - Utols√≥ 5 √ºzenet conversation context

Prompt instrukci√≥k:
1. Answer ONLY based on conversation summary and known facts
2. DO NOT make up or hallucinate information
3. Acknowledge limitation: "‚ö†Ô∏è V√°lasz korl√°tozott inform√°ci√≥k alapj√°n..."
4. If cannot answer ‚Üí suggest trying later or contacting team
5. Keep response concise and factual

Ultimate fallback (ha LLM is fail):
"‚ö†Ô∏è V√°lasz korl√°tozott inform√°ci√≥k alapj√°n (dokumentum retrieval √°tmenetileg nem el√©rhet≈ë):
Sajn√°lom, jelenleg nem tudok r√©szletes v√°laszt adni, mert a dokumentum-keres√©si 
rendszer √°tmenetileg nem el√©rhet≈ë. K√©rlek, pr√≥b√°ld √∫jra k√©s≈ëbb..."

Degradation stack:
graph TD
    A[User Query] --> B[Intent Detection]
    B --> C[RAG Retrieval + Timeout Wrapper]
    C -->|Success| D[Normal Generation with Citations]
    C -->|Timeout/Error| E[state rag_unavailable = True]
    E --> F[Summary-Only Generation]
    F --> G[Memory Summary + Facts + Last 5 Messages]
    G --> H[Warning Message + Limited Answer]
    H --> I[HTTP 503 Service Unavailable]

Degradation szintek:
Szint	Adatforr√°s	Citations	HTTP Status	User Notification
Normal	RAG + Memory	‚úÖ Yes	200 OK	-
Summary-Only	Memory + Context	‚ùå No	503 Service Unavailable	‚ö†Ô∏è "V√°lasz korl√°tozott inform√°ci√≥k alapj√°n..."
Ultimate Fallback	Hardcoded message	‚ùå No	503	‚ö†Ô∏è "Pr√≥b√°ld √∫jra k√©s≈ëbb..."

‚úÖ El≈ëny√∂k:
Non-blocking: RAG failure eset√©n sem failel az eg√©sz pipeline
Transparent: User-nek jelezz√ºk a korl√°toz√°st
Context-aware: Conversation history alapj√°n m√©g mindig tud v√°laszolni
Monitored: rag_unavailable flag + HTTP 503 status code
Fail-safe: Ultimate fallback ha m√©g az LLM is failel

üìù P√©lda v√°lasz RAG unavailable eset√©n:
‚ö†Ô∏è V√°lasz korl√°tozott inform√°ci√≥k alapj√°n (dokumentum retrieval √°tmenetileg nem el√©rhet≈ë):
Az el≈ëz≈ë besz√©lget√©s√ºnk alapj√°n eml√©kszem, hogy szabads√°gr√≥l k√©rdezt√©l. 
Sajnos most nem tudom ellen≈ërizni a pontos szabads√°g-politik√°t a HR dokumentumokban.
Javaslom:
- Pr√≥b√°ld √∫jra 5-10 perc m√∫lva
- Vedd fel k√∂zvetlen√ºl a kapcsolatot a HR csapattal
- Ellen≈ërizd a bels≈ë HR port√°lt

# idempotencia
Probl√©ma:
Ugyanaz a request t√∂bbsz√∂r elk√ºldve ‚Üí t√∂bbsz√∂ri LLM h√≠v√°s, k√∂lts√©g, duplik√°lt v√°laszok
Feedback endpoint-ban m√°r volt ON CONFLICT megold√°s
/api/regenerate/ endpoint m√°r haszn√°l cache-t, nem √ºtk√∂zhet

Megold√°s:
X-Request-ID header alap√∫ cache (Redis, TTL: 5 perc)
Client k√ºldi az X-Request-ID header-t (UUID v4)
Backend ellen≈ërzi Redis cache-ben: request_id:{id}
Ha HIT ‚Üí cached response vissza (X-Cache-Hit: true header)
Ha MISS ‚Üí norm√°l feldolgoz√°s, majd response cache-el√©se

El≈ëny√∂k:
‚úÖ Idempotens: Ugyanaz a request_id ‚Üí ugyanaz a v√°lasz
‚úÖ K√∂lts√©gcs√∂kkent√©s: Duplik√°lt requestek nem h√≠vnak LLM-et
‚úÖ Gyors v√°lasz: Cache hit eset√©n < 10ms latency
‚úÖ Kompatibilis: Nem √ºtk√∂zik a /api/regenerate/ endpoint-tal (m√°s cache kulcs)

# Memory Reducer Pattern + Szemantikus T√∂m√∂r√≠t√©s
Probl√©ma:
Eddig: Overwrite mode - Minden 8 √ºzenet ut√°n NEW summary (kor√°bbi elveszik)
Hossz√∫ besz√©lget√©sekn√©l az els≈ë N turn inform√°ci√≥ elv√©sz
Nincs semantikus sz≈±r√©s ‚Üí irrelev√°ns facts felhalmoz√≥dnak

Megold√°s - Reducer Pattern:
Previous Summary + New Messages ‚Üí LLM Merge ‚Üí Updated Summary
                                            ‚Üì
                                   Semantic Compression
                                            ‚Üì
                            Keep 8 MOST RELEVANT facts

Szemantikus T√∂m√∂r√≠t√©s (LLM-based filtering):
Merge similar facts: "user wants X" + "user needs X" ‚Üí "user requires X"
Prioritize recent over old (conflict resolution)
Keep domain constraints (dates, names, numbers)
Drop irrelevant facts (conversation direction changed)
Max 8 facts (compressed from prev_facts + new_messages)

Multi-level Summarization (j√∂v≈ëbeli extension):
Short (8 msg) ‚Üí medium (50 msg) ‚Üí long (200+ msg)
Tracked: total_msg_count v√°ltoz√≥
graph LR
    A[8 messages] --> B{Need Summary?}
    B -->|Yes| C[Load prev_summary + prev_facts]
    C --> D[LLM: Merge + Compress]
    D --> E[New Summary 3-5 sent]
    D --> F[8 MOST RELEVANT facts]
    E --> G[state.memory_summary = merged]
    F --> G

# idempontencia √©s memory reducer √∂sszefoglal√≥
El≈ëny√∂k:
‚úÖ Kumulat√≠v eml√©kezet: Kor√°bbi summary-k NEM vesznek el
‚úÖ Szemantikus filter: LLM d√∂nti el mi relev√°ns (nem fizikai limit)
‚úÖ Merge conflicts: Recent facts fel√ºl√≠rj√°k old-okat
‚úÖ Scalable: Long conversations ‚Üí multi-level summarization (later extension)
‚úÖ K√∂lts√©ghat√©kony: Max 8 fact t√°rol√°sa (nem 100+ fact)

Logging:
Memory updated (REDUCER): 6 facts (compressed from 13 items), 
summary length: 342 chars, total messages: 24

Idempotencia:
‚úÖ Ugyanaz a X-Request-ID ‚Üí instant cached response (<10ms)
‚úÖ K√ºl√∂nb√∂z≈ë X-Request-ID ‚Üí √∫j LLM h√≠v√°s
‚úÖ Nincs X-Request-ID ‚Üí norm√°l m≈±k√∂d√©s (backward compatible)
‚úÖ Response header tartalmazza: X-Cache-Hit: true (cache HIT eset√©n)

Memory Reducer:
‚úÖ T√∂bb turn conversation ‚Üí summary √∂sszef≈±z≈ëdik (nem overwrite)
‚úÖ Fact limit: Max 8 fact t√°rolva (nem t√∂bb)
‚úÖ Conflict resolution: √öjabb fact fel√ºl√≠rja r√©gebbi-t (pl. budget 50k ‚Üí 60k)
‚úÖ Log output: "Memory updated (REDUCER): X facts (compressed from Y items)"

# ===========================================================================
# ========== KRITIKUS BUGFIXEK (2026-01-21)
# ===========================================================================

## üêõ LangChain with_structured_output() Bug
**Probl√©ma**: `llm.with_structured_output(PydanticModel)` √ºres dict-eket (`{}`) adott vissza minden esetben
- √ârintett: IntentOutput, MemoryUpdate, ExecutionPlan, ToolSelection, ObservationOutput, RAGGenerationOutput
- Root cause: LangChain bug (API v√°ltoz√°s? verzi√≥ inkompatibilit√°s?)

**Megold√°s**: Manual JSON text parsing
```python
# R√©gi (NEM m≈±k√∂d√∂tt):
structured_llm = self.llm.with_structured_output(IntentOutput)
result = await structured_llm.ainvoke([HumanMessage(content=prompt)])
# ‚Üí result = {} (√ºres dict, NO KEYS!)

# √öj (M≈∞K√ñDIK):
prompt += '\n\nRespond with JSON: {"domain": "...", "confidence": 0.0, "reasoning": "..."}'
response = await self.llm.ainvoke([HumanMessage(content=prompt)])
response_text = response.content if hasattr(response, 'content') else str(response)

# JSON extraction (handles markdown code blocks)
json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
if json_match:
    json_str = json_match.group(1)
else:
    json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
    json_str = json_match.group(0) if json_match else '{}'

data = json.loads(json_str)
domain = data.get("domain", "general")
```

**Impact**:
- 6 node m√≥dos√≠tva: intent_detection, plan, tool_selection, observation, generation (2x)
- Prompt engineering: JSON format instrukci√≥val kieg√©sz√≠tve
- Regex-based extraction: Markdown code block vagy raw JSON t√°mogat√°s

---

## üêõ LangGraph State Management Violations

### Issue #1: Decision Functions Mutating State
**Probl√©ma**: Decision functions m√≥dos√≠tott√°k a state-et (FORBIDDEN in LangGraph)
```python
# ROSSZ (decision function):
def _observation_decision(self, state):
    state["replan_count"] = replan_count + 1  # ‚ùå ILLEGAL!
    return "replan"

# GraphRecursionError: Recursion limit of 25 reached
```

**Megold√°s**: State mutations moved to nodes
```python
# Decision function (READ-ONLY):
def _observation_decision(self, state):
    replan_count = state.get("replan_count", 0)
    if replan_count < 2:
        return "replan"
    return "generate"

# Plan node (STATE MUTATION):
async def _plan_node(self, state):
    replan_count = state.get("replan_count") or 0  # None-safe!
    if state.get("observation_result"):  # Coming from replan
        state["replan_count"] = replan_count + 1
    # ... continue planning
```

**Fixed Functions**:
- `_observation_decision`: Removed state["replan_count"] increment
- `_guardrail_decision`: Removed state["retry_count"] increment
- `_plan_node`: Added replan_count increment at start (if coming from observation)

---

### Issue #2: Node Name Conflicts
**Probl√©ma**: "observation" node name √ºtk√∂z√∂tt "observation" state field n√©vvel
```
'observation' is already being used as a state attribute, cannot also be used as a node name
```

**Megold√°s**:
- Node renamed: "observation" ‚Üí "observation_check"
- State field renamed: state["observation"] ‚Üí state["observation_result"]

---

### Issue #3: None-Safe replan_count
**Probl√©ma**: `state.get("replan_count", 0)` visszaadhat `None`-t (ha key l√©tezik `None` √©rt√©kkel)
```python
TypeError: '>' not supported between instances of 'NoneType' and 'int'
```

**Megold√°s**:
```python
# R√©gi:
replan_count = state.get("replan_count", 0)

# √öj (None-safe):
replan_count = state.get("replan_count") or 0  # Handles both missing and None
```

---

## üêõ GraphRecursionError: Recursion Limit
**Probl√©ma**: Default recursion_limit=25 t√∫l kicsi replanning-gel
- Complex queries: intent(1) + plan(1) + tools(2) + retrieval(2) + observation(2) + replan(1) + ... = ~24+ steps

**Megold√°s**: Increased to 50 in ainvoke config
```python
# Build graph:
return graph.compile()  # No recursion_limit here!

# Invoke with config:
final_state = await self.workflow.ainvoke(
    initial_state,
    config={"recursion_limit": 50}  # ‚Üê Config goes to ainvoke, NOT compile!
)
```

**Why 50?**
- Max 2 replans: ~24 steps
- Safety margin: 50 allows complex workflows
- Prevents infinite loops while supporting replanning

---

## üîß IT Domain: Auto-Append Jira Question
**Probl√©ma**: LLM nem mindig √≠rta oda a Jira ticket k√©rd√©st (prompt alap√∫, nem garant√°lt)

**Megold√°s**: Automatic append ha IT domain
```python
# In generation_node, after answer generation:
if is_it_domain:
    jira_question = "üìã Szeretn√©d, hogy l√©trehozzak egy Jira support ticketet ehhez a k√©rd√©shez?"
    if jira_question not in answer:
        answer = f"{answer}\n\n{jira_question}"
```

**Eredm√©ny**: MINDEN IT domain v√°lasz v√©g√©n megjelenik a k√©rd√©s (guaranteed)

---

## üìä Performance Note: 30+ sec Latency Normal
**Mi√©rt lass√∫?**
- 6-8 LLM h√≠v√°s / query (intent, plan, tool_selection, observation x2, generation, memory)
- OpenAI processing: 2-11 sec / h√≠v√°s
- Replanning: +2-3 extra h√≠v√°s
- Total: ~30-50 sec norm√°lis complex query-n√©l

**Optimaliz√°l√°s lehet≈ës√©gek** (j√∂v≈ëbeli):
- P√°rhuzamos tool calls (ahol f√ºgg≈ës√©g nincs)
- Prompt size optimaliz√°l√°s (kevesebb token ‚Üí gyorsabb)
- Streaming responses (UX jav√≠t√°s, nem gyors√≠t√°s)
- Gyorsabb modell (gpt-3.5-turbo), DE rosszabb min≈ës√©g!

# ===========================================================================
# ========== UNIT Tesztek el√©r√©se
# ===========================================================================

backend\tests