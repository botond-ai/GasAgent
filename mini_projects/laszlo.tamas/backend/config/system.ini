# System Configuration
# This file contains application-level settings that rarely change
# For environment-specific settings (credentials, hosts), use .env
#
# CONFIGURATION HIERARCHY (highest priority first):
# 1. .env file (deployment-specific overrides)
# 2. system.ini (version-controlled defaults)
# 3. Code defaults (fallback values)
#
# SEPARATION OF CONCERNS:
# - system.ini: Business logic, AI parameters, feature flags (version controlled)
# - .env: Secrets, infrastructure, ports, deployment overrides (NOT in git)

[application]
# Application version
APP_VERSION=0.6.2

# Language settings
DEFAULT_LANGUAGE=en

[development]
# Development mode - disables all caching layers for debugging
# WARNING: Set to false in production for optimal performance
DEV_MODE=true  # ENABLED for OpenAI prompt cache testing

[debug]
# Log outbound OpenAI HTTP request payloads to backend/debug/openai_outbound_payload.jsonl
# WARNING: Contains prompts and tool inputs; use only for debugging
LOG_OPENAI_OUTBOUND_PAYLOAD=true

# Nginx worker processes
# Development: frontend/nginx-main.conf sets worker_processes=2 (reduces health check logs)
# Production: Use 'auto' to match CPU cores for optimal performance
# Note: Each worker independently checks backend health, resulting in N health check requests

# Context window management
MAX_CONTEXT_TOKENS=8000

# Application-level system prompt (applies to ALL tenants and users)
# This is the base instruction set for the AI assistant
# Hierarchy: system.ini > tenant.system_prompt > user.system_prompt
SYSTEM_PROMPT = 

[llm]
# Dual-model architecture for cost optimization
# Heavy model: Final answers, complex reasoning, RAG synthesis (expensive but thorough)
# Light model: Routing, tool selection, extraction (70% cheaper, faster)
# Configuration moved to [llm.contexts] for function-specific optimization

# Explicit max_completion_tokens to include in OpenAI payload (cache stability)
MAX_COMPLETION_TOKENS=1500

# Use native OpenAI Chat Completions client (bypass LangChain serialization)
USE_NATIVE_OPENAI_CLIENT=true

# Chat history management (cache optimization)
# Maximum chat history messages included in prompt
# Benefits: Incremental cache hit rate increases with conversation length
# Trade-off: Beyond limit, oldest messages dropped (cache invalidation)
MAX_CHAT_HISTORY_MESSAGES=20

[llm.contexts]
# Function-specific LLM parameters (Model Pool Pattern)
# Each context optimized for its specific use case

# Light model contexts (gpt-4.1-mini: $0.40/$1.60 per 1M)
LIGHT_CHAT_TEMP=0.7
LIGHT_CHAT_MAX_TOKENS=500

LIGHT_ROUTER_TEMP=0.1
LIGHT_ROUTER_MAX_TOKENS=300

# Medium model contexts (gpt-4.1: $2.00/$8.00 per 1M)
MEDIUM_CHAT_TEMP=0.5
MEDIUM_CHAT_MAX_TOKENS=800

MEDIUM_RAG_TEMP=0.3
MEDIUM_RAG_MAX_TOKENS=1500

# Heavy model contexts (gpt-o1-mini: $1.10/$4.40 per 1M - reasoning model)
HEAVY_RAG_TEMP=0.2
HEAVY_RAG_MAX_TOKENS=1500

HEAVY_BIG_THINK_TEMP=0.1
HEAVY_BIG_THINK_MAX_TOKENS=4000

# Tool-specific instructions (optional contextual guidelines)
# These are injected only when relevant tools are called
# Set to empty string to disable

# RAG Answer Guidelines - injected ONLY after search_vectors or search_fulltext
# Purpose: Ensures complete sentences, avoids truncation issues
# NOTE: RAG guidelines are now loaded from external text files:
#       - backend/config/rag_guidelines_hu.txt
#       - backend/config/rag_guidelines_en.txt
ENABLE_RAG_GUIDELINES=true

[rag]
# RAG (Retrieval-Augmented Generation) pipeline settings
# FIGYELEM: Ezek a paraméterek NEM lehetnek hardcoded a kódban!

# Chunking
CHUNKING_STRATEGY=recursive
CHUNK_SIZE_TOKENS=500
CHUNK_OVERLAP_TOKENS=50

# Embedding
EMBEDDING_DIMENSIONS=3072
EMBEDDING_BATCH_SIZE=100

# Retrieval
TOP_K_DOCUMENTS=5
TOP_K_PRODUCTS=10
SIMILARITY_METRIC=cosine
MIN_SCORE_THRESHOLD=0.3

# Search mode settings
DEFAULT_SEARCH_MODE=hybrid
DEFAULT_VECTOR_WEIGHT=0.7
DEFAULT_KEYWORD_WEIGHT=0.3

# Qdrant search
QDRANT_SEARCH_LIMIT=10
QDRANT_SEARCH_OFFSET=0

# Qdrant upload batching (to avoid 32 MB payload limit)
# Lower this value if you get "Payload error: JSON payload is larger than allowed" errors
QDRANT_UPLOAD_BATCH_SIZE=50

[memory]
# Long-term memory settings
ENABLE_LONGTERM_CHAT_STORAGE=true
ENABLE_LONGTERM_CHAT_RETRIEVAL=false
CHAT_SUMMARY_MAX_TOKENS=200
CONSOLIDATE_AFTER_MESSAGES=50
MIN_MESSAGES_FOR_CONSOLIDATION=5

# Auto-consolidation idle timeout (in seconds)
# Frontend triggers consolidation after this period of inactivity  
# Default: 300 (5 minutes), Test: 60 (1 minute), DISABLED: 999999
IDLE_TIMEOUT_SECONDS=300

# Retrieval settings (for future implementation)
TOP_K_LONG_TERM_MEMORIES=3
MEMORY_SCORE_THRESHOLD=0.5

# Short-term memory (recent messages sent to LLM for context)
SHORT_TERM_MEMORY_MESSAGES=30
# Scope: 'session' = only current session messages | 'user' = last N messages across all user sessions
SHORT_TERM_MEMORY_SCOPE=user

[limits]
# Limits
MAX_FILE_SIZE_MB=10
MAX_CHUNKS_PER_DOCUMENT=1000
MAX_DOCUMENTS_PER_USER=100

[rate_limiting]
# API rate limits
REQUESTS_PER_MINUTE=60
MAX_CONCURRENT_REQUESTS=10

[features]
# Feature flags for optional functionality
# Enable/disable features without code changes for gradual rollout and A/B testing

# Query Rewrite - LLM-based query optimization before RAG search
# Benefits: Pronoun resolution, keyword expansion, context integration from chat history
# Cost: ~250-400ms latency, ~$0.0003 per request (light LLM)
# Can be overridden at runtime via API request parameter for A/B testing
QUERY_REWRITE_ENABLED=false

[pricing]
# OpenAI Model Pricing (2026-01-19, Standard tier)
# Source: https://platform.openai.com/docs/pricing
# All prices in USD per 1M tokens

# GPT-5 models
GPT5_NANO_INPUT_PRICE=0.05
GPT5_NANO_OUTPUT_PRICE=0.40
GPT5_NANO_CACHE_DISCOUNT=0.90

GPT5_MINI_INPUT_PRICE=0.25
GPT5_MINI_OUTPUT_PRICE=2.00
GPT5_MINI_CACHE_DISCOUNT=0.90

# GPT-4.1 models
GPT4_1_INPUT_PRICE=2.00
GPT4_1_OUTPUT_PRICE=8.00
GPT4_1_CACHE_DISCOUNT=0.75

GPT4_1_MINI_INPUT_PRICE=0.40
GPT4_1_MINI_OUTPUT_PRICE=1.60
GPT4_1_MINI_CACHE_DISCOUNT=0.75

# GPT-4o models
GPT4O_INPUT_PRICE=2.50
GPT4O_OUTPUT_PRICE=10.00
GPT4O_CACHE_DISCOUNT=0.50

GPT4O_MINI_INPUT_PRICE=0.15
GPT4O_MINI_OUTPUT_PRICE=0.60
GPT4O_MINI_CACHE_DISCOUNT=0.50

# GPT-3.5 models (no caching support)
GPT35_TURBO_INPUT_PRICE=0.50
GPT35_TURBO_OUTPUT_PRICE=1.50

# Embedding models
EMBEDDING_3_LARGE_INPUT_PRICE=0.13
EMBEDDING_3_SMALL_INPUT_PRICE=0.02
EMBEDDING_ADA_002_INPUT_PRICE=0.10

[cache]
# ============================================================================
# CACHE ARCHITECTURE - 4 LAYERS (see docs/03_implementations/CACHE_ARCHITECTURE.md)
# ============================================================================

# --- TIER 1: In-Memory Cache (SimpleCache) ---
# Python dict-based cache for ultra-fast lookups (<1ms)
# Location: backend/services/cache_service.py
# Cached data: system prompts, tenant/user metadata
ENABLE_MEMORY_CACHE=true
MEMORY_CACHE_TTL_SECONDS=3600
MEMORY_CACHE_DEBUG=false

# --- TIER 2: PostgreSQL Database Cache (user_prompt_cache table) ---
# Persistent cache that survives container restarts (~10ms latency)
# Location: backend/database/pg_init.py (user_prompt_cache table)
# Cached data: Built system prompts per user
ENABLE_DB_CACHE=true
DB_CACHE_AUTO_CLEANUP=false

# --- TIER 3: Browser Cache (Frontend HTTP cache) ---
# Controls Cache-Control headers sent to browser
# Location: frontend API calls + backend response headers
# Applies to: Static assets, API responses
ENABLE_BROWSER_CACHE=true
BROWSER_CACHE_MAX_AGE_SECONDS=300

# --- TIER 4: LLM Prompt Caching (OpenAI API - TEMP.4) ---
# Server-side caching at OpenAI (5-10 min ephemeral cache)
# Requires: GPT-4o model + cache_control parameter
ENABLE_LLM_CACHE=true
# NOTE: OpenAI prompt cache requires a minimum 1024-token identical prefix
LLM_CACHE_FORCE_SINGLE_MODEL=false
LLM_COMBINE_MESSAGES=true
LLM_PROMPT_CACHE_RETENTION=24h
LLM_PROMPT_CACHE_KEY_SCOPE=user

[resilience]
# API timeout and retry configuration
# Timeout values in seconds
OPENAI_TIMEOUT_SECONDS=60
QDRANT_TIMEOUT_SECONDS=30
DATABASE_TIMEOUT_SECONDS=30

# General retry logic for transient failures
# Used by @retry_with_backoff() decorator
MAX_RETRIES=3
BACKOFF_MULTIPLIER=2
INITIAL_BACKOFF_SECONDS=1
MAX_DELAY_SECONDS=10

# Rate limit specific retry settings
# Used by @retry_on_rate_limit() decorator for OpenAI API
RATE_LIMIT_MAX_ATTEMPTS=5
RATE_LIMIT_INITIAL_DELAY=2
RATE_LIMIT_MAX_DELAY=60
RATE_LIMIT_MULTIPLIER=2

# Quick retry settings for fast operations
# Used by @retry_quick() decorator for cache lookups
QUICK_RETRY_MAX_ATTEMPTS=2
QUICK_RETRY_INITIAL_DELAY=0.5

[logging]
# Detailed logging settings
LOG_LLM_REQUESTS=true
LOG_VECTOR_SEARCHES=true
LOG_EMBEDDING_OPERATIONS=true

[observability]
# ============================================================================
# OBSERVABILITY & MONITORING (Triple-Pillar Architecture)
# See: docs_user/MONITORING_IMPLEMENTATION_PLAN.md
# ============================================================================

# --- Metrics (Prometheus) ---
# Prometheus metrics collection for SLA monitoring and alerting
# Endpoint: /metrics (scraped every 15s by Prometheus)
ENABLE_METRICS=true
METRICS_DETAIL_LEVEL=standard  # minimal | standard | detailed

# --- Logs (Loki) ---
# Structured JSON logging with correlation IDs
# Push endpoint: Loki HTTP API (batched every 5s)
ENABLE_STRUCTURED_LOGGING=true
LOG_FORMAT=json  # json | text
LOG_LEVEL=INFO  # DEBUG | INFO | WARNING | ERROR
ENABLE_LOKI_PUSH=true
LOKI_BATCH_SIZE=100
LOKI_FLUSH_INTERVAL_SECONDS=5

# --- Traces (Tempo + OpenTelemetry) ---
# Distributed tracing for workflow execution analysis
# Export: OTLP protocol to Tempo
ENABLE_TRACES=true
TRACE_SAMPLING_RATIO=1.0  # 1.0 = 100% (dev), 0.1 = 10% (prod)
OTEL_SERVICE_NAME=knowledge-router-backend

# --- Workflow Tracking (PostgreSQL) ---
# Database-level execution audit and state timeline
# Tables: workflow_executions, node_executions
# Default config: workflow_tracking_config table (tenant_id=NULL)
# Per-tenant override: INSERT INTO workflow_tracking_config with tenant_id
# LOAD TEST: Temporarily disabled (heavy DB writes)
ENABLE_WORKFLOW_TRACKING=false
WORKFLOW_EXECUTION_RETENTION_DAYS=30

# Node-level tracking detail (can be overridden per tenant in DB)
# OFF: No node tracking (only workflow-level)
# METADATA_ONLY: Node name, duration, status (~2KB/node)
# FULL_STATE: Complete state snapshots before/after (~17KB/node)
NODE_TRACKING_LEVEL=METADATA_ONLY  # OFF | METADATA_ONLY | FULL_STATE

# --- Cost Tracking ---
# LLM cost calculation and aggregation
# Cost data stored in: Prometheus metrics (aggregated) + Loki logs (per-request)
ENABLE_COST_TRACKING=true
COST_ALERT_THRESHOLD_USD_PER_HOUR=10.0

# --- Retention & Cleanup ---
# Automatic data retention policies
PROMETHEUS_RETENTION_DAYS=15
LOKI_RETENTION_DAYS=30
TEMPO_RETENTION_DAYS=7

# --- Grafana Dashboard Time Windows ---
# Time range for rate() and increase() queries in dashboards
# Development: Use short windows (1m-2m) for immediate feedback
# Production: Use longer windows (5m-1h) for stability
GRAFANA_RATE_WINDOW=1m
GRAFANA_COST_WINDOW=30m

# --- Grafana Dashboard Mode ---
# Controls query style for immediate data visibility
# dev: Use instant queries (llm_requests_total) - immediate feedback, no historical rate calculation
# prod: Use rate queries (rate(llm_requests_total[1m])) - stable over time, requires scrape history
# Development: dev for immediate metric visibility after restart
# Production: prod for accurate rate calculations and trend analysis
GRAFANA_DASHBOARD_MODE=dev

# --- Grafana Continuous Logging ---
# Enable/disable continuous verbose logging for Grafana operations
# ON (true): Log all dashboard queries, data source requests, health checks
# OFF (false): Log only errors and warnings (recommended for production)
# Development: ON for debugging dashboard issues
# Production: OFF to reduce log volume
GRAFANA_CONTINUOUS_LOGGING=true


