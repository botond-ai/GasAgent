# AI Agent Demo - LangGraph + FastAPI + React

A complete working example demonstrating an AI Agent workflow with a Python backend (FastAPI + LangGraph) and React frontend.
The recent changes made are: 
- **Radio API Tool**: Fetches current radio station information and playing tracks for various countries and genres
- **Book Tool**: Provides RAG (Retrieval-Augmented Generation) capabilities for querying literary content, currently featuring Ferenc Moln√°r's "P√°l Utcai Fi√∫k"

## üéØ Overview

This application demonstrates the **Agent Workflow Cycle**:

```
Prompt ‚Üí Decision ‚Üí Tool ‚Üí Observation ‚Üí Memory ‚Üí Response
```

**Workflow**: `Agent ‚Üí Tool ‚Üí Agent ‚Üí User`

The agent uses **LangGraph** for orchestration, **OpenAI** for LLM capabilities, and provides a **ChatGPT-like interface** for interaction.

## ‚ú® Key Features

### Agent Capabilities
- **LangGraph-based orchestration**: Graph of nodes for agent reasoning and tool execution
- **7 integrated tools**:
  - üå§Ô∏è Weather forecast (Open-Meteo)
  - üó∫Ô∏è Geocoding and reverse geocoding (OpenStreetMap Nominatim)
  - üìç IP geolocation (ipapi.co)
  - üí± Foreign exchange rates (ExchangeRate.host)
  - ‚Çø Cryptocurrency prices (CoinGecko)
  - üìù File creation (local storage)
  - üîç Conversation history search
  - üìª New feature: Radio API, where you can ask stats about radio stations all over the world
  - üìö Book RAG: Query Ferenc Moln√°r's "P√°l Utcai Fi√∫k" using FAISS vector database

- **Memory management**: Maintains user preferences, conversation history, and workflow state
- **Multi-language support**: Responds in user's preferred language (Hungarian/English)

### Persistence
- ‚úÖ **All conversation messages** persisted to JSON files
- ‚úÖ **User profiles** stored separately (never deleted)
- ‚úÖ **Reset context** command: Clears conversation but preserves profile
- ‚úÖ **File-based storage**: Simple, transparent, and easy to inspect

### Architecture
- üèóÔ∏è **SOLID principles** applied throughout
- üì¶ **Clean architecture**: Domain ‚Üí Services ‚Üí Infrastructure ‚Üí API layers
- üîå **Dependency Inversion**: Abstract interfaces for all external dependencies
- üéØ **Single Responsibility**: Each class/module has one clear purpose
- üîì **Open/Closed**: Easy to extend with new tools without modifying existing code

## üèõÔ∏è Architecture

### Backend Structure

```
backend/
‚îú‚îÄ‚îÄ domain/                 # Domain layer - Core business entities
‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Data models (Message, UserProfile, Memory, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ interfaces.py      # Abstract interfaces (IUserRepository, IToolClient, etc.)
‚îú‚îÄ‚îÄ infrastructure/        # Infrastructure layer - External implementations
‚îÇ   ‚îú‚îÄ‚îÄ repositories.py    # File-based persistence (user profiles, conversations)
‚îÇ   ‚îî‚îÄ‚îÄ tool_clients.py    # External API clients (weather, crypto, FX, etc.)
‚îú‚îÄ‚îÄ services/              # Service layer - Business logic
‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # LangGraph agent implementation
‚îÇ   ‚îú‚îÄ‚îÄ tools.py           # Tool wrappers for agent
‚îÇ   ‚îî‚îÄ‚îÄ chat_service.py    # Chat workflow orchestration
‚îî‚îÄ‚îÄ main.py               # API layer - FastAPI endpoints
```

### LangGraph Workflow

The agent is implemented as a **LangGraph state graph**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  agent_decide   ‚îÇ  ‚Üê Entry: Analyzes request, decides action
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚Üí tool_weather ‚îÄ‚îÄ‚îê
         ‚îú‚îÄ‚Üí tool_geocode ‚îÄ‚îÄ‚î§
         ‚îú‚îÄ‚Üí tool_ip ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îú‚îÄ‚Üí tool_fx ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚Üí agent_finalize ‚îÄ‚Üí END
         ‚îú‚îÄ‚Üí tool_crypto ‚îÄ‚îÄ‚îÄ‚î§
         ‚îú‚îÄ‚Üí tool_file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îú‚îÄ‚Üí tool_radio ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îú‚îÄ‚Üí tool_book ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îú‚îÄ‚Üí tool_search ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îî‚îÄ‚Üí agent_finalize ‚îÄ‚Üí END (if no tool needed)
```

**Nodes**:
- `agent_decide`: LLM reasoning - decides whether to call tools
- `tool_*`: Individual tool execution nodes
- `agent_finalize`: Generates final natural language response

### Persistence Model

#### User Profile (`data/users/{user_id}.json`)
```json
{
  "user_id": "user_123",
  "language": "hu",
  "default_city": "Budapest",
  "created_at": "2025-12-08T10:00:00",
  "updated_at": "2025-12-08T10:30:00",
  "preferences": {}
}
```

**Behavior**:
- ‚úÖ Created automatically on first interaction
- ‚úÖ Updated when preferences change
- ‚ùå **Never deleted** - persists across all sessions

#### Conversation History (`data/sessions/{session_id}.json`)
```json
{
  "session_id": "session_456",
  "messages": [
    {
      "role": "user",
      "content": "What's the weather in Budapest?",
      "timestamp": "2025-12-08T10:15:00",
      "metadata": null
    },
    {
      "role": "system",
      "content": "Fetched weather forecast for location (47.4979, 19.0402)",
      "timestamp": "2025-12-08T10:15:01",
      "metadata": null
    },
    {
      "role": "assistant",
      "content": "A jelenlegi h≈ëm√©rs√©klet Budapesten 12¬∞C.",
      "timestamp": "2025-12-08T10:15:02",
      "metadata": null
    }
  ],
  "summary": null,
  "created_at": "2025-12-08T10:15:00",
  "updated_at": "2025-12-08T10:15:02"
}
```

**Behavior**:
- ‚úÖ All messages (user, assistant, system, tool) are persisted
- ‚úÖ Can be cleared with "reset context" command
- ‚úÖ User profile remains intact after reset

### Frontend Structure

```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatWindow.tsx      # Scrollable message list
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageBubble.tsx   # Individual message display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx       # User input field
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DebugPanel.tsx      # Tools & memory viewer
‚îÇ   ‚îú‚îÄ‚îÄ App.tsx                 # Main application
‚îÇ   ‚îú‚îÄ‚îÄ api.ts                  # Backend API client
‚îÇ   ‚îú‚îÄ‚îÄ types.ts                # TypeScript interfaces
‚îÇ   ‚îî‚îÄ‚îÄ utils.ts                # Utility functions
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ vite.config.ts
‚îî‚îÄ‚îÄ package.json
```

## üöÄ Getting Started

### Prerequisites
- **Python 3.11+**
- **Node.js 18+**
- **Docker & Docker Compose** (for containerized deployment)
- **OpenAI API Key**

### Option 1: Docker (Recommended)

1. **Clone and navigate**:
   ```bash
   cd ai_agent_complex
   ```

2. **Set up environment**:
   ```bash
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY
   ```

3. **Run with Docker Compose**:
   ```bash
   docker-compose up --build
   ```

4. **Access the application**:
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

### Option 2: Local Development

#### Backend

1. **Navigate to backend**:
   ```bash
   cd backend
   ```

2. **Create virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Set environment variable**:
   ```bash
   export OPENAI_API_KEY='your_api_key_here'
   # On Windows: set OPENAI_API_KEY=your_api_key_here
   ```

5. **Run the server**:
   ```bash
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

#### Frontend

1. **Navigate to frontend**:
   ```bash
   cd frontend
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Run development server**:
   ```bash
   npm run dev
   ```

4. **Access**: http://localhost:3000

## üìö API Endpoints

### `POST /api/chat`
Process chat message or reset context.

**Request**:
```json
{
  "user_id": "user_123",
  "message": "What's the weather in Budapest?",
  "session_id": "session_456"
}
```

**Response**:
```json
{
  "final_answer": "A jelenlegi h≈ëm√©rs√©klet Budapesten 12¬∞C.",
  "tools_used": [
    {
      "name": "weather",
      "arguments": {"city": "Budapest"},
      "success": true
    }
  ],
  "memory_snapshot": {
    "preferences": {
      "language": "hu",
      "default_city": "Budapest"
    },
    "workflow_state": {
      "flow": null,
      "step": 0,
      "total_steps": 0
    },
    "message_count": 3
  },
  "logs": ["Tools called: 1"]
}
```

### `GET /api/session/{session_id}`
Get conversation history.

### `GET /api/profile/{user_id}`
Get user profile.

### `PUT /api/profile/{user_id}`
Update user profile.

**Request**:
```json
{
  "language": "en",
  "default_city": "Szeged"
}
```

### `GET /api/history/search?q=weather`
Search conversation history.

## üí° Example Interactions

### Weather Query
```
User: What will the weather be like tomorrow in Budapest?
Agent: [Calls geocode tool ‚Üí weather tool]
Response: A holnap el≈ërejelzett h≈ëm√©rs√©klet Budapesten 8-14¬∞C k√∂z√∂tt lesz.
```

### Cryptocurrency Price
```
User: What's the current BTC price in EUR?
Agent: [Calls crypto_price tool]
Response: A Bitcoin (BTC) jelenlegi √°ra 42,350 EUR, 24 √≥r√°s v√°ltoz√°s: +2.3%.
```

### Language Preference Update
```
User: From now on, answer in English
Agent: [Updates user profile]
Response: Understood! I will respond in English from now on.
```

### Reset Context
```
User: reset context
Agent: [Clears conversation history, keeps profile]
Response: Context has been reset. We are starting a new conversation, but your preferences are preserved.
```

### History Search
```
User: Search our past conversations for 'weather'
Agent: [Calls search_history tool]
Response: I found 3 previous mentions of weather in our conversations...
```

## üé® Special Features

### Reset Context Command
When a user sends `"reset context"` (case-insensitive):
1. ‚úÖ Conversation history is **cleared**
2. ‚úÖ User profile is **preserved**
3. ‚úÖ New session starts fresh
4. ‚úÖ Preferences (language, city) remain intact

**Implementation**: Detected in `ChatService.process_message()` before agent invocation.

### User Profile Management
- **Never deleted**: Only created/loaded and updated
- **Automatic updates**: Agent detects preference changes in conversation
- **Manual updates**: Via `PUT /api/profile/{user_id}` endpoint
- **Persistent across sessions**: Stored in `data/users/{user_id}.json`

### Memory Context
The agent receives:
- **Recent messages**: Last 20 messages for context
- **User preferences**: Language, default city, custom preferences
- **Workflow state**: Multi-step process tracking (extensible)

## üèóÔ∏è SOLID Principles Applied

### Single Responsibility Principle (SRP)
- Each class/module has **one clear purpose**
- `FileUserRepository`: Only handles user profile persistence
- `WeatherTool`: Only handles weather API calls
- `ChatService`: Only orchestrates chat workflow

### Open/Closed Principle (OCP)
- **Easy to add new tools** without modifying existing code
- New tool: Implement `IToolClient`, create wrapper in `tools.py`, register in `agent.py`
- **No changes needed** to agent core logic or graph structure

### Liskov Substitution Principle (LSP)
- All tool clients implement `IToolClient` interface
- Can be swapped without breaking agent functionality
- Mock implementations for testing

### Interface Segregation Principle (ISP)
- **Specific interfaces** for different concerns:
  - `IUserRepository`: User profile operations
  - `IConversationRepository`: Conversation operations
  - `IWeatherClient`, `IFXRatesClient`, etc.: Specific tool operations
- Clients only depend on methods they use

### Dependency Inversion Principle (DIP)
- High-level modules (`ChatService`, `AIAgent`) depend on **abstractions** (`IUserRepository`, `IToolClient`)
- Low-level modules (repositories, API clients) implement abstractions
- **Easy to swap implementations** (file storage ‚Üí database, real APIs ‚Üí mocks)

## üõ†Ô∏è Technologies

### Backend
- **FastAPI**: Modern async web framework
- **LangGraph**: Agent orchestration and workflow
- **LangChain**: LLM integration utilities
- **OpenAI**: GPT-4 for reasoning and responses
- **Pydantic**: Data validation and settings
- **httpx**: Async HTTP client for tools

### Frontend
- **React 18**: UI library
- **TypeScript**: Type-safe JavaScript
- **Vite**: Fast build tool
- **Axios**: HTTP client
- **CSS**: Custom ChatGPT-like styling

### Infrastructure
- **Docker**: Containerization
- **Docker Compose**: Multi-container orchestration
- **Nginx**: Static file serving and reverse proxy
- **JSON files**: Simple, transparent persistence

## üìÇ Data Storage

All data is stored in JSON files for transparency and easy inspection:

```
data/
‚îú‚îÄ‚îÄ users/           # User profiles (never deleted)
‚îÇ   ‚îî‚îÄ‚îÄ user_123.json
‚îú‚îÄ‚îÄ sessions/        # Conversation histories (can be reset)
‚îÇ   ‚îî‚îÄ‚îÄ session_456.json
‚îî‚îÄ‚îÄ files/           # User-created files
    ‚îî‚îÄ‚îÄ user_123/
        ‚îî‚îÄ‚îÄ note.txt
```

## üß™ Development

### Backend Tests
```bash
cd backend
pytest  # (Add tests in tests/ directory)
```

### Frontend Tests
```bash
cd frontend
npm test  # (Add tests with Vitest/Jest)
```

### Type Checking
```bash
cd frontend
npm run type-check
```

## üîí Environment Variables

### Required
- `OPENAI_API_KEY`: Your OpenAI API key

### Optional
- Backend runs on port `8000` by default
- Frontend runs on port `3000` by default
- Adjust in `docker-compose.yml` or locally

## üöß Extending the Application

### Adding a New Tool

1. **Create client** in `infrastructure/tool_clients.py`:
   ```python
   class MyAPIClient(IToolClient):
       async def execute(self, **kwargs) -> Dict[str, Any]:
           # Implementation
   ```

2. **Create tool wrapper** in `services/tools.py`:
   ```python
   class MyTool:
       def __init__(self, client: MyAPIClient):
           self.client = client
           self.name = "my_tool"
           self.description = "..."
       
       async def execute(self, **kwargs) -> Dict[str, Any]:
           # Wrapper logic
   ```

3. **Register in agent** (`services/agent.py`):
   ```python
   self.tools["my_tool"] = my_tool_instance
   ```

4. **Add to graph** (automatic via node creation in `_build_graph`)

### Adding a New Workflow Step

Modify `WorkflowState` in `domain/models.py` and update `ChatService` logic to track multi-step processes.

## üìù License

This is a demo application for educational purposes.

## ü§ù Contributing

This is a teaching example. Feel free to fork and extend for your own learning!

---

**Built with ‚ù§Ô∏è for the AI Agent Programming Course**
