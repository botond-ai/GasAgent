Tesztkérdések:

1. Mi a különbség a hibrid keresés és a tisztán vektoros keresés között, és mikor melyiket érdemes választani?
2. Miért szükséges a LLM szövegkereső agent esetén az overlap a chunkok között, és mekkora legyen tipikusan százalékosan?
3. Milyen metadata mezők a leghasznosabbak a RAG adatbázidokban, LLM segítségével történő szűrt visszakereséshez és auditáláshoz?
4. Hogyan mérnéd a RAG adatbázisból, LLM felhasználásával elvégzett retrieval minőséget, ha nincs címkézett tanító/adatod?
5. Miért javasolt kétlépcsős retrieval (recall-orientált első kör + re-ranking)?
6. Mik a leggyakoribb hibaminták, ha a modell “talál, mégis rosszul válaszol”?
7. Hogyan kezelnéd a LLM-el támogatott tudásbáziokban a dokumentumok verziózását és a régi chunkok “kiszorítását” az indexből?
8. Mit jelent a groundedness, és hogyan kényszerítenéd ki, hogy az LLM által adott válasz csak forrásokból dolgozzon?
9. Mikor érdemes query rewritinget és multi-queryt használni, és hogyan hat ez a recallra?
10. Hogyan kezelnéd a táblázatos adatokat chunkoláskor, hogy a sorok önmagukban is érthetők legyenek az LLM-el történő feldolgozáshoz?