Meeting Date: 2023-10-28
Topic: Incident Post-Mortem - Production Outage
Attendees: Dave (SRE), Elena (Lead Dev), Sam (CTO)

Sam: Thank you for hopping on. Let's review what happened during the outage yesterday. Dave, can you walk us through the timeline?
Dave: Sure. At 14:00 UTC, we started receiving alerts for high latency on the checkout service. By 14:05, error rates spiked to 15%. We declared an incident at 14:07.
Elena: I dug into the logs. It looks like a database migration lock caused a backlog of write requests. The migration was supposed to be non-blocking.
Sam: Why did we think it was non-blocking?
Elena: The documentation for the ORM suggested that adding a column with a default value in this version of Postgres plays nice, but because the table size is fairly large (20M rows), it still took a lock that lasted longer than our transaction timeouts.
Dave: We mitigated it by killing the migration process at 14:15. System recovered by 14:18.
Sam: Usage of `alembic` or raw SQL?
Elena: It was an autogenerated alembic script.
Sam: Action items?
Dave: We need to enforce a policy that all migrations on tables over 1M rows are reviewed by a DBA or SRE.
Elena: And we should test migrations on a clone of production data, not just a small staging set.
Sam: Agreed. Let's also update our playbook for dealing with locked tables. Good catch and quick resolution, team.
